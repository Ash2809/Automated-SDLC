{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9dcc3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\a\\envs\\env2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\a\\envs\\env2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, Dict, List, Optional, Literal, TypedDict\n",
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c78517",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", \n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056c0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello, world!\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19523bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Gemini:\n",
    "    def __init__(self):\n",
    "        self.model = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            api_key=GOOGLE_API_KEY,\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "    def complete(self, system: str, user: str, **kwargs) -> str:\n",
    "        messages = []\n",
    "        if system.strip():\n",
    "            messages.append({\"role\": \"system\", \"content\": system.strip()})\n",
    "        messages.append({\"role\": \"user\", \"content\": user.strip()})\n",
    "\n",
    "        resp = self.model.invoke(messages)\n",
    "        return resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "\n",
    "\n",
    "LLM = Gemini()\n",
    "print(LLM.complete(system=\"\", user=\"Hello\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ecec32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Constraint:\n",
    "    tech_stack : Optional[str] = None\n",
    "    deployment : Optional[str]= None\n",
    "    priority : Optional[Literal[\"high\", \"medium\", \"low\"]] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fe09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProjectSpec:\n",
    "    project_name : str\n",
    "    description : str\n",
    "    features : List[str]\n",
    "    constraints : Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92836620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorState(TypedDict, total = False):\n",
    "    spec = Dict[str, Any]\n",
    "\n",
    "    #Artifacts \n",
    "    user_stories: List[Dict[str, Any]]\n",
    "    architecture : Dict[str, Any]\n",
    "    plan : List[Dict[str, Any]]\n",
    "    code_changes : Dict[str, str]\n",
    "    tests : Dict[str, str]\n",
    "    test_report : Dict[str, str]\n",
    "    ci_cd : Dict[str, str]\n",
    "    deployment : Dict[str, str]\n",
    "\n",
    "    #these are for logging each nde in langgraph\n",
    "    approvals : Dict[str, Any]\n",
    "    repo_url : Optional[str]\n",
    "    run_id : Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea02b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HitlProvider:\n",
    "    def request(self, gate: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class CLIHitl(HitlProvider):\n",
    "    def request(self, gate: str, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(\"\\n===== HITL GATE:\", gate, \"=====\")\n",
    "\n",
    "        print(json.dumps(payload, indent=2)[:4000])\n",
    "\n",
    "        while True:\n",
    "            ans = input(\"Approve? [y/n/e=edit]: \").strip().lower()\n",
    "            if ans in {\"y\", \"n\", \"e\"}:\n",
    "                break\n",
    "\n",
    "        notes = \"\"\n",
    "\n",
    "        if ans == \"e\":\n",
    "            print(\"Enter revised JSON (end with an empty line):\")\n",
    "            buf = []\n",
    "            while True:\n",
    "                line = input()\n",
    "                if not line:\n",
    "                    break\n",
    "                buf.append(line)\n",
    "            try:\n",
    "                revised = json.loads(\"\\n\".join(buf))\n",
    "                payload = revised\n",
    "            except Exception as e:\n",
    "                print(\"Invalid JSON, keeping original:\", e)\n",
    "        \n",
    "        elif ans == \"n\":\n",
    "            notes = input(\"Reason for rejection? \")\n",
    "        \n",
    "        return {\"approved\": ans == \"y\", \"payload\": payload, \"notes\": notes, \"by\": \"cli-user\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d47b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitl_provider : HitlProvider = CLIHitl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ab5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spec(raw : Dict[str, Any]) -> ProjectSpec:\n",
    "    constraints = raw.get(\"constraints\") or {}\n",
    "    return ProjectSpec(\n",
    "        project_name = raw.get(\"project_name\", \"Untitled Project\"),\n",
    "        description = raw.get(\"description\", \"\"),\n",
    "        features = raw.get(\"features\", []),\n",
    "        constraints= Constraint(\n",
    "            tech_stack = constraints.get(\"tech_stack\"),\n",
    "            deployment = constraints.get(\"deployment\"),\n",
    "            priority = constraints.get(\"priority\")\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73bba564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(data : Any) -> str:\n",
    "    return json.dumps(data, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3225d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_json(text: str):\n",
    "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            return {}\n",
    "    return {}\n",
    "# import json\n",
    "# import re\n",
    "\n",
    "# def extract_json(text: str) -> dict:\n",
    "#     \"\"\"Extract JSON from LLM response, handling code fences safely.\"\"\"\n",
    "#     try:\n",
    "#         # Strip outer ```json fences\n",
    "#         cleaned = re.sub(r\"^```[a-zA-Z]*\\n?\", \"\", text.strip())\n",
    "#         cleaned = re.sub(r\"```$\", \"\", cleaned.strip())\n",
    "\n",
    "#         # Parse JSON\n",
    "#         data = json.loads(cleaned)\n",
    "\n",
    "    #     # If values are strings with ```python fences, strip them too\n",
    "    #     if isinstance(data, dict):\n",
    "    #         for k, v in data.items():\n",
    "    #             if isinstance(v, str):\n",
    "    #                 v = re.sub(r\"^```[a-zA-Z]*\\n?\", \"\", v.strip())\n",
    "    #                 v = re.sub(r\"```$\", \"\", v.strip())\n",
    "    #                 data[k] = v\n",
    "\n",
    "    #     return data\n",
    "    # except Exception as e:\n",
    "    #     print(\"âŒ extract_json failed:\", e)\n",
    "    #     return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3367811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def requirements_node(state : OrchestratorState) -> OrchestratorState:\n",
    "#     spec = normalize_spec(state[\"spec\"])\n",
    "#     prompt_sys = (\n",
    "#         \"You are a senior product manager. Convert the input spec into precise, testable user stories. \"\n",
    "#         \"Use INVEST. Include acceptance criteria and non-functional requirements when relevant.\"\n",
    "#     )\n",
    "\n",
    "#     prompt_user = f\"SPEC:\\n{to_json(asdict(spec))}\\nReturn json with fields: user_stories[].\"\n",
    "    \n",
    "#     text = LLM.complete(system=prompt_sys, user=prompt_user)\n",
    "#     print(\"LLM response:\", text[:4000])\n",
    "\n",
    "#     try:\n",
    "#         data = json.loads(text)\n",
    "#         stories = data.get(\"user_stories\", [])\n",
    "#     except Exception:\n",
    "#         stories = []\n",
    "#     return {**state, \"user_stories\": stories}\n",
    "    \n",
    "def requirements_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    spec = normalize_spec(state[\"spec\"])\n",
    "    prompt_sys = (\n",
    "        \"You are a senior product manager. Convert the input spec into precise, testable user stories. \"\n",
    "        \"Use INVEST. Include acceptance criteria and non-functional requirements when relevant.\"\n",
    "    )\n",
    "    prompt_user = f\"SPEC:\\n{to_json(asdict(spec))}\\nReturn JSON with fields: user_stories[].\"\n",
    "\n",
    "    text = LLM.complete(system=prompt_sys, user=prompt_user)\n",
    "    # print(\"LLM response (truncated):\", text[:1000])\n",
    "\n",
    "    # Extract the first valid JSON object\n",
    "    data = extract_json(text)\n",
    "    stories = data.get(\"user_stories\", [])\n",
    "    return {**state, \"user_stories\": stories}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2a68812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"spec\": {\n",
      "    \"project_name\": \"Task Manager\",\n",
      "    \"description\": \"A simple task management app\",\n",
      "    \"features\": [\n",
      "      \"add task\",\n",
      "      \"list tasks\",\n",
      "      \"mark task done\"\n",
      "    ],\n",
      "    \"constraints\": {\n",
      "      \"tech_stack\": \"Python + FastAPI\"\n",
      "    }\n",
      "  },\n",
      "  \"user_stories\": [\n",
      "    {\n",
      "      \"id\": \"US001\",\n",
      "      \"title\": \"As a user, I want to add a new task to the task manager so that I can keep track of my to-dos.\",\n",
      "      \"description\": \"Allows users to add tasks with a description and optional due date.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"The system shall allow users to input a task description (text field, minimum 1 character).\",\n",
      "        \"The system shall allow users to optionally input a due date (date picker).\",\n",
      "        \"The system shall display a success message upon successful task creation.\",\n",
      "        \"The system shall handle invalid input (e.g., empty description) gracefully with appropriate error messages.\",\n",
      "        \"The newly added task should be immediately visible in the task list.\"\n",
      "      ],\n",
      "      \"priority\": \"High\",\n",
      "      \"estimated_effort\": \"3 days\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"US002\",\n",
      "      \"title\": \"As a user, I want to see a list of all my tasks so that I can review my to-dos.\",\n",
      "      \"description\": \"Displays a list of all tasks, including their description and due date (if any).\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"The system shall display a list of all tasks added by the user.\",\n",
      "        \"Each task in the list shall display its description.\",\n",
      "        \"Each task in the list shall display its due date (if set).\",\n",
      "        \"The list shall be sortable by due date (ascending and descending).\",\n",
      "        \"The list shall be visually clear and easy to read.\"\n",
      "      ],\n",
      "      \"priority\": \"High\",\n",
      "      \"estimated_effort\": \"2 days\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"US003\",\n",
      "      \"title\": \"As a user, I want to mark a task as done so that I can track my progress.\",\n",
      "      \"description\": \"Allows users to mark a task as complete.\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"The system shall provide a mechanism (e.g., checkbox, button) to mark a task as complete.\",\n",
      "        \"Upon marking a task as done, its status should be updated accordingly.\",\n",
      "        \"Completed tasks should be visually distinct from incomplete tasks (e.g., strikethrough, different color).\",\n",
      "        \"The system should prevent marking a task as done more than once.\"\n",
      "      ],\n",
      "      \"priority\": \"High\",\n",
      "      \"estimated_effort\": \"2 days\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample_state = {\n",
    "        \"spec\": {\n",
    "            \"project_name\": \"Task Manager\",\n",
    "            \"description\": \"A simple task management app\",\n",
    "            \"features\": [\"add task\", \"list tasks\", \"mark task done\"],\n",
    "            \"constraints\": {\"tech_stack\": \"Python + FastAPI\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = requirements_node(sample_state)\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ba99e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitl_gate(state : OrchestratorState, gate : str, payload_keys : List[str]) -> OrchestratorState:\n",
    "    payload = {k: state.get(k) for k in payload_keys}\n",
    "    decision = hitl_provider.request(gate, payload)\n",
    "    \n",
    "    approvals = state.get(\"approvals\", {})\n",
    "    approvals[gate] = {k: v for k, v in decision.items() if k != \"payload\"}\n",
    "    \n",
    "    #allowing the human edits\n",
    "    updated = {**state, **(decision.get(\"payload\", {}))}\n",
    "    updated[\"approvals\"] = approvals\n",
    "    if not decision.get(\"approved\", True):\n",
    "        raise Exception(f\"Gate {gate} rejected: {decision.get('notes', 'No reason provided')}\")\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04e37a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_node(state : OrchestratorState) -> OrchestratorState:\n",
    "    spec = normalize_spec(state[\"spec\"])\n",
    "\n",
    "    prompt_sys = (\n",
    "        \"You are a pragmatic software architect. Propose a simple, scalable architecture. \"\n",
    "        \"Include: system diagram (text), services, data model, API contracts, and key trade-offs.\"\n",
    "    )\n",
    "    prompt_user = (\n",
    "        f\"PROJECT: {spec.project_name}\\nDESC: {spec.description}\\nFEATURES: {to_json(spec.features)}\\n\"\n",
    "        f\"CONSTRAINTS: {to_json(asdict(spec.constraints))}\\nReturn JSON field 'architecture'.\"\n",
    "    )\n",
    "\n",
    "    text = LLM.complete(system=prompt_sys, user=prompt_user)\n",
    "    # print(\"LLM raw response:\", text[:100])\n",
    "\n",
    "    data = extract_json(text)\n",
    "    arch = data.get(\"architecture\", {})\n",
    "\n",
    "    return {**state, \"architecture\": arch}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3330aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Output ===\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"project_name\": \"Task Manager\",\n",
      "    \"description\": \"A simple task management app\",\n",
      "    \"features\": [\n",
      "      \"add task\",\n",
      "      \"list tasks\",\n",
      "      \"mark task done\"\n",
      "    ],\n",
      "    \"constraints\": {\n",
      "      \"tech_stack\": \"Python + FastAPI\"\n",
      "    }\n",
      "  },\n",
      "  \"architecture\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample_state = {\n",
    "        \"spec\": {\n",
    "            \"project_name\": \"Task Manager\",\n",
    "            \"description\": \"A simple task management app\",\n",
    "            \"features\": [\"add task\", \"list tasks\", \"mark task done\"],\n",
    "            \"constraints\": {\"tech_stack\": \"Python + FastAPI\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = design_node(sample_state)\n",
    "    print(\"=== Final Output ===\")\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fe89855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planning_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    prompt_sys = (\n",
    "        \"You are an agile tech lead. Turn user stories and architecture into a plan: \"\n",
    "        \"epics, tasks, estimates, and dependencies.\"\n",
    "    )\n",
    "    \n",
    "    prompt_user = (\n",
    "        f\"Stories: {to_json(state.get('user_stories', []))}\\n\"\n",
    "        f\"Arch: {to_json(state.get('architecture', {}))}\\n\"\n",
    "        f\"Return JSON with field 'plan' as a list.\"\n",
    "    )\n",
    "\n",
    "    text = LLM.complete(system=prompt_sys, user=prompt_user)\n",
    "    # print(\"LLM raw response (truncated):\", text[:10])  \n",
    "\n",
    "    data = extract_json(text)\n",
    "    plan = data.get(\"plan\", [])\n",
    "\n",
    "    return {**state, \"plan\": plan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6b10e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Plan Output ===\n",
      "{\n",
      "  \"spec\": {\n",
      "    \"project_name\": \"Task Manager\",\n",
      "    \"description\": \"A simple task management app\",\n",
      "    \"features\": [\n",
      "      \"add task\",\n",
      "      \"list tasks\",\n",
      "      \"mark task done\"\n",
      "    ],\n",
      "    \"constraints\": {\n",
      "      \"tech_stack\": \"Python + FastAPI\"\n",
      "    }\n",
      "  },\n",
      "  \"user_stories\": [\n",
      "    {\n",
      "      \"id\": \"US001\",\n",
      "      \"title\": \"As a user, I want to add a task\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"User can enter a task description\",\n",
      "        \"Task is saved and visible in task list\"\n",
      "      ],\n",
      "      \"priority\": \"High\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"US002\",\n",
      "      \"title\": \"As a user, I want to list my tasks\",\n",
      "      \"acceptance_criteria\": [\n",
      "        \"System shows all tasks with descriptions\",\n",
      "        \"Tasks can be sorted by due date\"\n",
      "      ],\n",
      "      \"priority\": \"High\"\n",
      "    }\n",
      "  ],\n",
      "  \"architecture\": {\n",
      "    \"services\": [\n",
      "      {\n",
      "        \"name\": \"API Gateway\",\n",
      "        \"description\": \"Handles incoming API requests\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Task Service\",\n",
      "        \"description\": \"Manages task CRUD operations\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"plan\": [\n",
      "    {\n",
      "      \"epic\": \"Task Management\",\n",
      "      \"stories\": [\n",
      "        \"US001\",\n",
      "        \"US002\"\n",
      "      ],\n",
      "      \"tasks\": [\n",
      "        {\n",
      "          \"id\": \"T001\",\n",
      "          \"title\": \"Design Task API endpoints\",\n",
      "          \"description\": \"Define API endpoints for task creation and retrieval in the API Gateway.\",\n",
      "          \"story\": \"US001, US002\",\n",
      "          \"service\": \"API Gateway\",\n",
      "          \"estimate\": 2,\n",
      "          \"dependencies\": []\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T002\",\n",
      "          \"title\": \"Implement Task API endpoints\",\n",
      "          \"description\": \"Implement API endpoints for task creation and retrieval in the API Gateway, routing to the Task Service.\",\n",
      "          \"story\": \"US001, US002\",\n",
      "          \"service\": \"API Gateway\",\n",
      "          \"estimate\": 4,\n",
      "          \"dependencies\": [\n",
      "            \"T001\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T003\",\n",
      "          \"title\": \"Develop Task Service CRUD operations\",\n",
      "          \"description\": \"Implement Create, Read, Update, and Delete operations for tasks in the Task Service.\",\n",
      "          \"story\": \"US001, US002\",\n",
      "          \"service\": \"Task Service\",\n",
      "          \"estimate\": 8,\n",
      "          \"dependencies\": []\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T004\",\n",
      "          \"title\": \"Implement Task creation UI\",\n",
      "          \"description\": \"Create a user interface for adding new tasks.\",\n",
      "          \"story\": \"US001\",\n",
      "          \"service\": \"UI\",\n",
      "          \"estimate\": 4,\n",
      "          \"dependencies\": [\n",
      "            \"T002\",\n",
      "            \"T003\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T005\",\n",
      "          \"title\": \"Implement Task listing UI\",\n",
      "          \"description\": \"Create a user interface to display the list of tasks, sortable by due date.\",\n",
      "          \"story\": \"US002\",\n",
      "          \"service\": \"UI\",\n",
      "          \"estimate\": 6,\n",
      "          \"dependencies\": [\n",
      "            \"T002\",\n",
      "            \"T003\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T006\",\n",
      "          \"title\": \"Testing (API Gateway)\",\n",
      "          \"description\": \"Unit and integration tests for API Gateway endpoints.\",\n",
      "          \"story\": \"US001, US002\",\n",
      "          \"service\": \"API Gateway\",\n",
      "          \"estimate\": 2,\n",
      "          \"dependencies\": [\n",
      "            \"T002\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T007\",\n",
      "          \"title\": \"Testing (Task Service)\",\n",
      "          \"description\": \"Unit and integration tests for Task Service.\",\n",
      "          \"story\": \"US001, US002\",\n",
      "          \"service\": \"Task Service\",\n",
      "          \"estimate\": 4,\n",
      "          \"dependencies\": [\n",
      "            \"T003\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"id\": \"T008\",\n",
      "          \"title\": \"UI Testing\",\n",
      "          \"description\": \"End-to-end testing of the UI.\",\n",
      "          \"story\": \"US001, US002\",\n",
      "          \"service\": \"UI\",\n",
      "          \"estimate\": 4,\n",
      "          \"dependencies\": [\n",
      "            \"T004\",\n",
      "            \"T005\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample_state = {\n",
    "        \"spec\": {\n",
    "            \"project_name\": \"Task Manager\",\n",
    "            \"description\": \"A simple task management app\",\n",
    "            \"features\": [\"add task\", \"list tasks\", \"mark task done\"],\n",
    "            \"constraints\": {\"tech_stack\": \"Python + FastAPI\"}\n",
    "        },\n",
    "        \"user_stories\": [\n",
    "            {\n",
    "                \"id\": \"US001\",\n",
    "                \"title\": \"As a user, I want to add a task\",\n",
    "                \"acceptance_criteria\": [\n",
    "                    \"User can enter a task description\",\n",
    "                    \"Task is saved and visible in task list\"\n",
    "                ],\n",
    "                \"priority\": \"High\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"US002\",\n",
    "                \"title\": \"As a user, I want to list my tasks\",\n",
    "                \"acceptance_criteria\": [\n",
    "                    \"System shows all tasks with descriptions\",\n",
    "                    \"Tasks can be sorted by due date\"\n",
    "                ],\n",
    "                \"priority\": \"High\"\n",
    "            }\n",
    "        ],\n",
    "        \"architecture\": {\n",
    "            \"services\": [\n",
    "                {\"name\": \"API Gateway\", \"description\": \"Handles incoming API requests\"},\n",
    "                {\"name\": \"Task Service\", \"description\": \"Manages task CRUD operations\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = planning_node(sample_state)\n",
    "\n",
    "    print(\"=== Final Plan Output ===\")\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae073f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implementation_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    spec = normalize_spec(state[\"spec\"])\n",
    "    prompt_sys = (\n",
    "        \"You are a senior engineer. Generate minimal, production-ready code skeletons with docstrings and TODOs. \"\n",
    "        \"Prefer simplicity and testability.\"\n",
    "    )\n",
    "    \n",
    "    prompt_user = (\n",
    "    f\"Tech stack: {spec.constraints.tech_stack}\\nPlan: {to_json(state.get('plan', []))}\\n\"\n",
    "    \"Output JSON with key 'code_changes' as {filename: content}. \"\n",
    "    \"IMPORTANT: Return raw JSON only. Do not wrap code in markdown fences (no ```python, no ```). \"\n",
    "    \"Each file's content must be a plain string value inside JSON.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    text = LLM.complete(prompt_sys, prompt_user)\n",
    "    print(\"LLM raw response (truncated):\", text[:500])  \n",
    "\n",
    "    try:\n",
    "        data = extract_json(text)\n",
    "        code_changes = data.get(\"code_changes\", {})\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON:\", e)\n",
    "        code_changes = {}\n",
    "\n",
    "    return {**state, \"code_changes\": code_changes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7983735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM raw response (truncated): ```json\n",
      "{\n",
      "  \"code_changes\": {\n",
      "    \"app/main.py\": \"'''\\nFastAPI application.\\n'''\\n\\nfrom fastapi import FastAPI, HTTPException\\nfrom pydantic import BaseModel\\n\\napp = FastAPI()\\n\\nclass Task(BaseModel):\\n    id: int\\n    description: str\\n    completed: bool = False\\n\\n# TODO: Replace with a persistent storage (e.g., database)\\ntasks = []\\n\\n@app.post(\\\"/tasks/\\\")\\ndef add_task(task: Task):\\n    \\\"\\\"\\\"Add a new task.\\n    \\\"\\\"\\\"\\n    # TODO: Handle potential errors (e.g., duplicate ID)\\n    tas\n",
      "=== Final Code Changes ===\n",
      "\n",
      "--- app/main.py ---\n",
      "'''\n",
      "FastAPI application.\n",
      "'''\n",
      "\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class Task(BaseModel):\n",
      "    id: int\n",
      "    description: str\n",
      "    completed: bool = False\n",
      "\n",
      "# TODO: Replace with a persistent storage (e.g., database)\n",
      "tasks = []\n",
      "\n",
      "@app.post(\"/tasks/\")\n",
      "d...\n",
      "\n",
      "\n",
      "--- app/tests/test_main.py ---\n",
      "'''\n",
      "Tests for the FastAPI application.\n",
      "'''\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from app.main import app\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "def test_add_task():\n",
      "    \"\"\"Test adding a new task.\n",
      "    \"\"\"\n",
      "    # TODO: Implement test logic\n",
      "    pass\n",
      "\n",
      "def test_list_tasks():\n",
      "    \"\"\"Test listing ta...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample_state = {\n",
    "        \"spec\": {\n",
    "            \"project_name\": \"Task Manager\",\n",
    "            \"description\": \"A simple task management app\",\n",
    "            \"features\": [\"add task\", \"list tasks\", \"mark task done\"],\n",
    "            \"constraints\": {\"tech_stack\": \"Python + FastAPI\"}\n",
    "        },\n",
    "        \"plan\": [\n",
    "            {\n",
    "                \"epic\": \"Task CRUD\",\n",
    "                \"tasks\": [\n",
    "                    {\"id\": \"T001\", \"description\": \"Implement add task API\", \"estimate\": \"2d\"},\n",
    "                    {\"id\": \"T002\", \"description\": \"Implement list tasks API\", \"estimate\": \"1d\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    result = implementation_node(sample_state)\n",
    "\n",
    "    print(\"=== Final Code Changes ===\")\n",
    "    for filename, content in result.get(\"code_changes\", {}).items():\n",
    "        print(f\"\\n--- {filename} ---\\n{content[:300]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c705a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_node(state : OrchestratorState) -> OrchestratorState:\n",
    "    prompt_sys = (\n",
    "        \"You are a test engineer. Generate unit & integration tests to cover core behaviors.\"\n",
    "    )\n",
    "    prompt_user = (\n",
    "        f\"Given code files: {list((state.get('code_changes') or {}).keys())}\\n\"\n",
    "        \"Return JSON with 'tests' (filename->content) and 'test_plan' summary.\"\n",
    "    )\n",
    "\n",
    "    text = LLM.complete(prompt_sys, prompt_user)\n",
    "\n",
    "    print(\"LLM raw response (truncated):\", text[:500])\n",
    "\n",
    "    tests: Dict[str, str]\n",
    "    test_plan: Dict[str, Any]\n",
    "\n",
    "    try:\n",
    "        data = extract_json(text)\n",
    "        tests = data.get(\"tests\", {})\n",
    "        test_plan = data.get(\"test_plan\", {})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse:\", e)\n",
    "        tests, test_plan = {}, {}\n",
    "\n",
    "\n",
    "    report = {\"summary\": \"stubbed\", \"passed\": True, \"coverage\": 0.0, \"notes\": \"Replace with real runner\"}\n",
    "    return {**state, \"tests\": tests, \"test_plan\": test_plan, \"test_report\": report}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29f3fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM raw response (truncated): ```json\n",
      "{\n",
      "  \"tests\": {\n",
      "    \"test_api_tasks.py\": \"import unittest\\nimport json\\nfrom unittest.mock import patch\\nfrom api.tasks import create_task, get_task, update_task, delete_task\\n\\n# Sample Task Data\\nsample_task = {'id': 1, 'title': 'Test Task', 'description': 'This is a test task', 'completed': False}\\n\\nclass TestTasksAPI(unittest.TestCase):\\n\\n    @patch('api.tasks.db') # Mocking database interaction\\n    def test_create_task(self, mock_db):\\n        mock_db.insert_one.return_value = {'i\n",
      "\n",
      "=== Generated Test Plan ===\n",
      "Unit Tests:\n",
      "\n",
      "* **create_task:** Tests successful task creation, verifies data integrity, and checks database interaction using mocking.\n",
      "* **get_task:** Tests successful retrieval of a task, handles cases where the task is not found, and verifies database interaction.\n",
      "* **update_task:** Tests successful task update, handles cases where the task is not found, and verifies database interaction.\n",
      "* **delete_task:** Tests successful task deletion, handles cases where the task is not found, and verifies database interaction.\n",
      "\n",
      "Integration Tests (Commented out):\n",
      "\n",
      "* **test_integration_create_get_delete:** (To be implemented) This test would involve setting up a real database, creating a task, retrieving it, and deleting it to verify the end-to-end functionality.  It's currently commented out because it requires a running database and is more complex to set up and maintain.  Uncomment to run when a database is available.\n",
      "\n",
      "Note:  The unit tests use `unittest.mock.patch` to mock database interactions, making them fast and independent of a database server.  The integration test is a placeholder and should be implemented separately once a database is available.  The integration test would provide more confidence in the overall system behavior.\n",
      "\n",
      "=== Generated Tests ===\n",
      "\n",
      "--- test_api_tasks.py ---\n",
      "import unittest\n",
      "import json\n",
      "from unittest.mock import patch\n",
      "from api.tasks import create_task, get_task, update_task, delete_task\n",
      "\n",
      "# Sample Task Data\n",
      "sample_task = {'id': 1, 'title': 'Test Task', 'description': 'This is a test task', 'completed': False}\n",
      "\n",
      "class TestTasksAPI(unittest.TestCase):\n",
      "\n",
      "    @...\n",
      "\n",
      "\n",
      "=== Test Report (stubbed) ===\n",
      "{'summary': 'stubbed', 'passed': True, 'coverage': 0.0, 'notes': 'Replace with real runner'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # simulate state coming out of implementation_node\n",
    "    sample_state = {\n",
    "        \"spec\": {\n",
    "            \"project_name\": \"Task Manager\",\n",
    "            \"description\": \"A simple task management app\",\n",
    "            \"features\": [\"add task\", \"list tasks\", \"mark task done\"],\n",
    "            \"constraints\": {\"tech_stack\": \"Python + FastAPI\"}\n",
    "        },\n",
    "        \"code_changes\": {\n",
    "            \"api/tasks.py\": \"\"\"\\\"\\\"\\\"Tasks API\\\"\\\"\\\"\n",
    "from fastapi import APIRouter\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "@router.get(\"/tasks\")\n",
    "def list_tasks():\n",
    "    return []  # TODO: implement\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = testing_node(sample_state)\n",
    "\n",
    "    print(\"\\n=== Generated Test Plan ===\")\n",
    "    print(result.get(\"test_plan\", {}))\n",
    "\n",
    "    print(\"\\n=== Generated Tests ===\")\n",
    "    for filename, content in result.get(\"tests\", {}).items():\n",
    "        print(f\"\\n--- {filename} ---\\n{content[:300]}...\\n\")  # preview first 300 chars\n",
    "\n",
    "    print(\"\\n=== Test Report (stubbed) ===\")\n",
    "    print(result.get(\"test_report\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deployment_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    spec = normalize_spec(state[\"spec\"])  \n",
    "\n",
    "    prompt_sys = (\n",
    "        \"You are a DevOps engineer. Produce CI/CD config and containerization files. \"\n",
    "        \"Return valid JSON only, no markdown code fences, no explanations.\"\n",
    "    )\n",
    "\n",
    "    prompt_user = (\n",
    "        f\"Deployment: {getattr(spec.constraints, 'deployment', 'unspecified')}\\n\"\n",
    "        \"Output JSON with:\\n\"\n",
    "        \"  - 'ci_cd': {filename: content}, where content is plain text (no markdown fences)\\n\"\n",
    "        \"  - 'deployment': {strategy, envs}\"\n",
    "    )\n",
    "\n",
    "    text = LLM.complete(prompt_sys, prompt_user)\n",
    "    print(\"LLM raw response (truncated):\", text[:500])\n",
    "\n",
    "    try:\n",
    "        cleaned = text.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = re.sub(r\"^```(json)?\", \"\", cleaned, flags=re.IGNORECASE).strip()\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned[:-3].strip()\n",
    "\n",
    "        data = json.loads(cleaned)\n",
    "        ci_cd = data.get(\"ci_cd\", {})\n",
    "        deployment = data.get(\"deployment\", {})\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse deployment JSON:\", e)\n",
    "        ci_cd, deployment = {}, {}\n",
    "\n",
    "    return {**state, \"ci_cd\": ci_cd, \"deployment\": deployment}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "600f4034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM raw response (truncated): ```json\n",
      "{\n",
      "  \"ci_cd\": {\n",
      "    \"filename\": \".github/workflows/main.yml\",\n",
      "    \"content\": \"name: CI/CD\\n\\non: \\n  push:\\n    branches:\\n      - main\\n\\njobs:\\n  build:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n      - name: Build the Docker image\\n        run: docker build -t my-app:latest .\\n      - name: Log in to Docker Hub\\n        run: docker login -u ${{ secrets.DOCKERHUB_USERNAME }} -p ${{ secrets.DOCKERHUB_PASSWORD }}\\n      - name: Push the Docker image\\n    \n",
      "=== Generated CI/CD Configs ===\n",
      "\n",
      "--- filename ---\n",
      ".github/workflows/main.yml...\n",
      "\n",
      "\n",
      "--- content ---\n",
      "name: CI/CD\n",
      "\n",
      "on: \n",
      "  push:\n",
      "    branches:\n",
      "      - main\n",
      "\n",
      "jobs:\n",
      "  build:\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "      - uses: actions/checkout@v3\n",
      "      - name: Build the Docker image\n",
      "        run: docker build -t my-app:latest .\n",
      "      - name: Log in to Docker Hub\n",
      "        run: docker login -u ${{ secrets.D...\n",
      "\n",
      "=== Deployment Strategy ===\n",
      "{'strategy': 'Docker + Kubernetes', 'envs': {'DOCKERHUB_USERNAME': 'your_dockerhub_username', 'DOCKERHUB_PASSWORD': 'your_dockerhub_password'}}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sample_state = {\n",
    "        \"spec\": {\n",
    "            \"project_name\": \"Task Manager\",\n",
    "            \"description\": \"A simple task management app\",\n",
    "            \"features\": [\"add task\", \"list tasks\", \"mark task done\"],\n",
    "            \"constraints\": {\n",
    "                \"tech_stack\": \"Python + FastAPI\",\n",
    "                \"deployment\": \"Docker + GitHub Actions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = deployment_node(sample_state)\n",
    "\n",
    "    print(\"=== Generated CI/CD Configs ===\")\n",
    "    for filename, content in result.get(\"ci_cd\", {}).items():\n",
    "        print(f\"\\n--- {filename} ---\\n{content[:300]}...\\n\")  # preview first 300 chars\n",
    "\n",
    "    print(\"=== Deployment Strategy ===\")\n",
    "    print(result.get(\"deployment\", {}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2ce0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph() -> Any:\n",
    "    graph = StateGraph(OrchestratorState)\n",
    "\n",
    "    graph.add_node(\"requirements\", requirements_node)\n",
    "    graph.add_node(\"design\", design_node)\n",
    "    graph.add_node(\"planning\", planning_node)\n",
    "    graph.add_node(\"implementation\", implementation_node)\n",
    "    graph.add_node(\"testing\", testing_node)\n",
    "    graph.add_node(\"deployment\", deployment_node)\n",
    "\n",
    "    graph.add_node(\"hitl_requirements\", lambda s: hitl_gate(s, \"requirements_review\", [\"user_stories\"]))\n",
    "    graph.add_node(\"hitl_design\", lambda s: hitl_gate(s, \"architecture_review\", [\"architecture\"]))\n",
    "    graph.add_node(\"hitl_preprod\", lambda s: hitl_gate(s, \"preprod_release\", [\"test_report\", \"ci_cd\"]))\n",
    "    graph.add_node(\"hitl_prod\", lambda s: hitl_gate(s, \"prod_release\", [\"deployment\"]))\n",
    "\n",
    "    graph.set_entry_point(\"requirements\")\n",
    "    graph.add_edge(\"requirements\", \"hitl_requirements\")\n",
    "    graph.add_edge(\"hitl_requirements\", \"design\")\n",
    "    graph.add_edge(\"design\", \"hitl_design\")\n",
    "    graph.add_edge(\"hitl_design\", \"planning\")\n",
    "    graph.add_edge(\"planning\", \"implementation\")\n",
    "    graph.add_edge(\"implementation\", \"testing\")\n",
    "    graph.add_edge(\"testing\", \"hitl_preprod\")\n",
    "    graph.add_edge(\"hitl_preprod\", \"deployment\")\n",
    "    graph.add_edge(\"deployment\", \"hitl_prod\")\n",
    "    graph.add_edge(\"hitl_prod\", END)\n",
    "\n",
    "    checkpointer = MemorySaver()\n",
    "    return graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "53391d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'deployment' is already being used as a state key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 9\u001b[0m, in \u001b[0;36mbuild_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m, implementation_node)\n\u001b[0;32m      8\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m, testing_node)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeployment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeployment_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhitl_requirements\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m s: hitl_gate(s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequirements_review\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_stories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     12\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhitl_design\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m s: hitl_gate(s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchitecture_review\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\a\\envs\\env2\\lib\\site-packages\\langgraph\\graph\\state.py:336\u001b[0m, in \u001b[0;36mStateGraph.add_node\u001b[1;34m(self, node, action, metadata, input, retry)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode name must be provided if action is not a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels:\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already being used as a state key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled:\n\u001b[0;32m    338\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding a node to a graph that has already been compiled. This will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be reflected in the compiled graph.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: 'deployment' is already being used as a state key"
     ]
    }
   ],
   "source": [
    "build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c675ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
